---
title: "Machine Learning and Weight Lifting Action Classification Prediction"
author: "Wong Wing Keng"
date: "December 2015"
output:
  html_document:
    css: custom.css
    keep_md: yes
    toc: yes
  pdf_document: default
  word_document: default
subtitle: Practical Machine Learning Assignment
---

----  

###Executive Summary

This report will explore the **Weight Lifting Exercise Dataset** from [ [ Velloso et al., 2013 ] ](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) and attempts to build machine learning models. The best model is then chosen to be used to predict the manner in which a group of participants who performed barbell lifting exercise.  The prediction model will segregate their actions according to a pre-defined classification set i.e., Class A to E (based upon their weight lifting movement). The predicted result can possibly be further utilised to measure/reviews the correctness of the weight lifting exercise.

Additional reference: http://groupware.les.inf.puc-rio.br/har 
(*see the section on the Weight Lifting Exercise Dataset).  

In this report, different prediction models were tested. Each model's training process, prediction accuracy and out of sample errors were compared. The best performing model observed is the model which uses the **"Random Forest"** algorithm. The **"Random Forest"** model achieved an overall accuracy of **99.27%**. The detailed data exploration, model training and accuracy reviews are shown below. 

----  

###Background Information
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity relatively inexpensively.     

In the Weight Lifting dataset, 6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The five ways are exactly:

* According to the specification (Class A)  
* Throwing the elbows to the front (Class B)  
* Lifting the dumbbell only halfway (Class C)  
* Lowering the dumbbell only halfway (Class D)     
* Throwing the hips to the front (Class E)   

*Note: Only Class A corresponds to correct performance. 

Data were collected from accelerometers that were placed on the belt, forearm, arm, and dumbell of the participants. These formed the basis of the Weight Lifting data. 

----

###Data Loading and Basic Exploratory

2 pre-defined set of the weight lifting dataset (one for training set and the other as testing set) are made available online and will be used in the report.

####Training data set

Downloading and loading training dataset

```{r load_train, echo=TRUE, cache=TRUE}
if(!dir.exists("./data")){dir.create("./data")}
training_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(training_url, destfile="./data/pml-training.csv", method="auto")
training <- read.csv("./data/pml-training.csv", na.strings = c("NA", ""))
```

####Testing data set

Downloading and loading testing dataset

```{r load_test, echo=TRUE, cache=TRUE}
testing_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(testing_url, destfile="./data/pml-testing.csv", method="auto")
testing <- read.csv("./data/pml-testing.csv", na.strings = c("NA", ""))
```

```{r dim1, echo=TRUE}
dim(training); dim(testing)
```

Both raw dataset has **`r ncol(training)`** columns. Differences is training dataset contains **`r nrow(training)`** rows while testing dataset contains only **`r nrow(testing)`** rows. 

High level review of the data structure (first 20 columns) shows the following:  

```{r str_listing, echo=TRUE}
str(training, list.len=20)

###Sample check on total number of NAs
summary(training$max_picth_belt)
summary(training$avg_roll_belt)
```

\* It can be observed that multiple columns in the raw dataset actually contains empty data (**"NA"**s values). Random checks (see sample above) on these columns shows huge number of rows (>50% of total number of rows) are actually empty.

Additional check on zero covariates:

```{r nzerolist, cache=TRUE, echo=TRUE}
###Near Zero Covariates
suppressMessages(library(caret))
nzlist<-nearZeroVar(training) ###Near Zero Variance Column Index
length(nzlist)
```

There are  **`r length(nzlist)`** features/columns that have very few unique values in the raw dataset. These would also required further processing.

----

###Data Cleaning and Further Exploration

The previously identified data columns with only few unique values ( or small variances) are considered to be bad predictors for usage in machine learning model building. These data columns will be removed.

```{r remove_nzlist} 
training<-training[, -nzlist]
testing<-testing[, -nzlist] # synchronize the same removal action on testing dataset
```

Since prediction cannot handle missing/empty data hence any remaining data columns with **NA** values will also be removed from the datasets. Data imputation is not performed as the amount of missing data is simply too big to have meaningful value replacement.

```{r remove_na, echo=TRUE}
nalist<-colnames(training[, colSums(is.na(training)) != 0]) ### Retrieve the column names
naindex<-which(names(training) %in% nalist) ### Retrieve the column indexes
training<-training[, -naindex] ###Remove the columns
testing<-testing[, -naindex] ###Remove the same columns from testing set
dim(training); dim(testing)
```

Cleaned datasets now contains **`r ncol(training)`** columns.  

```{r disp_names, echo=TRUE}
names(training)
```

The objective of the report is on predicting the classification of actions, variable/column **"classe"** is thus the desired **outcome**. The remaining data columns will be treated as the **predictors**. By reducing the number of predictors/columns it should helps improves the speed of further model training as irrelevant data are being removed.

```{r head_disp, echo=TRUE}
head(training,2)
```

Further review on the remaining columns shows the first **6** variables/columns (referencing the **"head"** function output above) do not contain information/values that would necessary influences nor helps in identifying the weight lifting direction and hence the **"classe"**" outcome. These 6 columns will be removed as well.

```{r remove_variable, echo=TRUE}
training<-training[,-c(1:6)]
testing<-testing[,-c(1:6)] 
dim(training); dim(testing)
```

Final refined datasets now contains only **`r ncol(training)`** columns/variables.

----

###Cross Validation Dataset Creation

The training dataset contains **`r nrow(training)`** rows of observations. This is considered to be a reasonably large set, it can be further divided into smaller subsets to help cross validation on model building and training. Hence further subdividision is performed. It is separated into a smaller training and validation set.

```{r split_validation, echo=TRUE}
suppressMessages(library(caret))
set.seed(3323) ### Setting randomizer seed for reproducibility research

trainIndex <- createDataPartition(training$classe, p = 0.6, list = FALSE) ### 60/40 split
train_dat <- training[trainIndex,]
valid_dat  <- training[-trainIndex,]
dim(train_dat);dim(valid_dat)
```

2 new datasets are now created for model training and cross validation purposes.

----  

###Algorithms and Model Training

In the following section, several machine learning algorithm is tested with model training.  

In order to speed up model training, parallel processing functionality is enabled. In addition, the model training options is tuned to achieve a standardized comparable and assumed acceptable balance between machine model training duration and accuracy level.

```{r model_testing_prep, echo=TRUE}
### Enabling parallel processing to speed up model training
suppressMessages(library(doParallel))
cl <- makeCluster(detectCores()) ### detect and set num of CPU Cores to use
registerDoParallel(cl) ### Registering the parallel processes

### Storage arrays for keeping model testing results
model_list<-list() ### Storing model
train_time_elapse<-list() ### storing training timing details
pred_time_elapse<-list() ### storing prediction timing details
accuracy_level<-list() ### storing confusionMatrix result

### model training control arguments: repeated cross validation, 3 folds
cntrl <- trainControl(method="repeatedcv", number=3)
```

----    

####Model 1: Recursive Partitioning and Regression Trees Model  

```{r model1, echo=TRUE, cache=TRUE}
### Model 1: Recursive Partitioning and Regression Trees Model
suppressMessages(library(rpart))
train_time_elapse[[1]]<-system.time(model_list[[1]] <- train(classe ~ ., data = train_dat,method = "rpart",trControl =cntrl ))

print(model_list[[1]])
model_list[[1]]$finalModel
```

Visually viewing the decision tree model:  

```{r rpart_plot, echo=TRUE}
suppressMessages(library(rattle))
suppressMessages(library(rpart))
fancyRpartPlot(model_list[[1]]$finalModel, sub="Model 1 Decision Tree")
```

```{r predict1, echo=TRUE}
### Prediction againts validation dataset 
pred_time_elapse[[1]]<-system.time(pred1<-predict(model_list[[1]], valid_dat))

### Evaluate accuracy of model
accuracy_level[[1]] <- confusionMatrix(pred1, valid_dat$classe)
accuracy_level[[1]]
```

Model 1 using **rpart** shows a rather dissapointing accurancy level with only **`r round(100*accuracy_level[[1]]$overall[1],2)`%**; out of sample error recorded at **`r round(100*(1-accuracy_level[[1]]$overall[1]),2)`%**.  

----  

####Model 2: Random Forests Model  

```{r model2, echo=TRUE, cache=TRUE}
## Model 2: Random Forests Model
suppressMessages(library(randomForest))
train_time_elapse[[2]]<-system.time(model_list[[2]] <- train(classe ~ ., data = train_dat,method = "rf",trControl =cntrl ))

print(model_list[[2]])
model_list[[2]]$finalModel
```

```{r rf_plots, echo=TRUE}
suppressMessages(library(randomForest))
varImp(model_list[[2]])
plot(model_list[[2]])
```

The above plot shows **Random Forest** algorithm produces the highest accuracy result when it uses **27** out to the total **52** predictors supplied. (*Total 27 variables tried at each tree split as mentioned in the finalModel output).  

\*Although the **Random Forest** model has shown that only 27 predictors are required for model buiding but no further manual intervention/pre-processing will be performed on the raw dataset. Reason being the **Random Forest** algorithm itself will automatically determine the optimum combination of the 27 required predictors. This will be based upon the variables importance order as listed above. 

```{r predict2, echo=TRUE}
### Prediction againts validation dataset 
suppressMessages(library(randomForest))
pred_time_elapse[[2]]<-system.time(pred2<-predict(model_list[[2]], valid_dat))

### Evaluate accuracy of model
accuracy_level[[2]] <- confusionMatrix(pred2, valid_dat$classe)
accuracy_level[[2]]
```

Model 2 using **Random Forest** shows a very high accuracy level with **`r round(100*accuracy_level[[2]]$overall[1],2)`%**; out of sample error recorded at **`r round(100*(1-accuracy_level[[2]]$overall[1]),2)`%**.

----  

####Model 3: Generalized Boosted Regression Models  

```{r model3, echo=TRUE, cache=TRUE}
## Model 3: Generalized Boosted Regression Models
suppressMessages(library(survival))
suppressMessages(library(plyr))
suppressMessages(library(gbm))
train_time_elapse[[3]]<-system.time(model_list[[3]] <- train(classe ~ ., data = train_dat,method = "gbm",trControl =cntrl, verbose=FALSE ))

print(model_list[[3]])
model_list[[3]]$finalModel
```

```{r gbm_plots, echo=TRUE}
plot(model_list[[3]])
```

The plot above shows as the **number of boosting iteration** and **max tree depth** increases the accuracy of the model will increases as well. In theory, it is therefore possible to further boost the current **gbm** prediction model by further tuning the model training control parameters. However this would not be done as to maintain the same conditions for all models and in order to ensure accurate result comparison later. In addition, there is also risk to overfitting if the current **gbm** model is being further tuned to fit the training dataset.   

```{r predict3, echo=TRUE}
### Prediction againts validation dataset 
suppressMessages(library(survival))
suppressMessages(library(plyr))
suppressMessages(library(gbm))
pred_time_elapse[[3]]<-system.time(pred3<-predict(model_list[[3]], valid_dat))

### Evaluate accuracy of model
accuracy_level[[3]] <- confusionMatrix(pred3, valid_dat$classe)
accuracy_level[[3]]
```

Model 3 using **Generalized Boosted Regression Models (gbm)** also shows a relatively high accuracy level with **`r round(100* accuracy_level[[3]]$overall[1],2)`%**; out of sample error recorded at **`r round(100*(1-accuracy_level[[3]]$overall[1]),2)`%**.

----  

####Model 4: Linear Discriminant Analysis  

```{r model4, echo=TRUE, cache=TRUE}
## Model 4: Linear Discriminant Analysis
suppressMessages(library(MASS))
train_time_elapse[[4]]<-system.time(model_list[[4]] <- train(classe ~ ., data = train_dat,method = "lda",trControl =cntrl ))

print(model_list[[4]])
#model_list[[4]]$finalModel ### Disabled - Optional display of extensive final model listing
```

```{r predict4, echo=TRUE}
### Prediction againts validation dataset 
suppressMessages(library(MASS))
pred_time_elapse[[4]]<-system.time(pred4<-predict(model_list[[4]], valid_dat))

### Evaluate accuracy of model
accuracy_level[[4]] <- confusionMatrix(pred4, valid_dat$classe)
accuracy_level[[4]]

### Stop and closing parallel processing
stopCluster(cl)
```

Model 4 using **Linear Discriminant Analysis (lda)** shows moderate accuracy level at **`r round(100* accuracy_level[[4]]$overall[1],2)`%**; out of sample error recorded at **`r round(100*(1-accuracy_level[[4]]$overall[1]),2)`%**.  

----  

###Model Comparison  

Compiled high level overview of all 4 tested models:   

####A) Model Results Summarization Table  

```{r compare_result, echo=TRUE}
###Compilling all results into tabular format
suppressMessages(library(knitr))

comparison_table<-data.frame(
   "Model.Algorithm" =c("a) Classification Tree **(rpart)**",
              "b) Random Forest **(rf)**",
              "c) Gradient Boosting Machine **(gbm)**",
              "d) Linear Discriminant Analysis **(lda)**"),
   "Model.Training.Speed"=c(train_time_elapse[[1]]['elapsed'],
                       train_time_elapse[[2]]['elapsed'],
                       train_time_elapse[[3]]['elapsed'],
                       train_time_elapse[[4]]['elapsed']),
   "Prediction.Speed"=c(pred_time_elapse[[1]]['elapsed'],
                       pred_time_elapse[[2]]['elapsed'],
                       pred_time_elapse[[3]]['elapsed'],
                       pred_time_elapse[[4]]['elapsed']),
   "Accuracy"=100 * c(accuracy_level[[1]]$overall[1],
                      accuracy_level[[2]]$overall[1],
                      accuracy_level[[3]]$overall[1],
                      accuracy_level[[4]]$overall[1]),
   "Out.of.Sample.Error.Rate"=100* c(1-accuracy_level[[1]]$overall[1],
                                     1-accuracy_level[[2]]$overall[1],
                                     1-accuracy_level[[3]]$overall[1],
                                     1-accuracy_level[[4]]$overall[1]))

names(comparison_table) <- c('Model Algorithm', 'Training Speed (sec)','Prediction Speed (sec)','Accuracy %', 'Out of Sample Error Rate %')

### Applied together with custom css style to create nice html table
kable(comparison_table,digits=2, align=c("l","c","c","c","c"), caption="**Model Comparison Summarization Table:**")

```

####B) Visual Review of Prediction Result on Validation Dataset  

```{r prediction_result_plot, fig.width=12, fig.height=8}
###Plotting prediction result visual reviews
suppressMessages(library(ggplot2))
suppressMessages(library(grid))
suppressMessages(library(gridExtra))

### Find matching result between prediction and actual in validation dataset
pred_Right1<-pred1==valid_dat$classe
pred_Right2<-pred2==valid_dat$classe
pred_Right3<-pred3==valid_dat$classe
pred_Right4<-pred4==valid_dat$classe

###qplots using color conding for TRUE/FALSE status
p1<-qplot(roll_belt, pitch_forearm, col=pred_Right1, data=valid_dat, main="Model 1 \"rpart\" \n Validation Dataset Prediction Result", size = I(3), alpha = I(0.7))

p2<-qplot(roll_belt, pitch_forearm, col=pred_Right2, data=valid_dat, main="Model 2 \"rf\" \n Validation Dataset Prediction Result",size = I(3), alpha = I(0.7))

p3<-qplot(roll_belt, pitch_forearm, col=pred_Right3, data=valid_dat, main="Model 3 \"gbm\" \n Validation Dataset Prediction Result",size = I(3), alpha = I(0.7))

p4<-qplot(roll_belt, pitch_forearm, col=pred_Right4, data=valid_dat, main="Model 4 \"lda\" \n Validation Dataset Prediction Result",size = I(3), alpha = I(0.7))

###Arrange plots in 2x2 grid 
grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
print(p1, vp = viewport(layout.pos.row = 1, layout.pos.col = 1)) 
print(p2, vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
print(p3, vp = viewport(layout.pos.row = 2, layout.pos.col = 1))
print(p4, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))
grid.rect(.5,.5,width=unit(.99,"npc"), height=unit(0.99,"npc"), 
          gp=gpar(lwd=2, fill=NA, col="grey"))
```


By picking the top 2 important variables: **roll_belt** and **pitch_forearm** (reference VarImp listing in model 2) and plotting againts each other, the prediction results from all 4 models are visually presented. Color coding is used to represet whether the predicted classe values matched to the original classe values from the validation dataset. 

###Conclusion

From the summarized table and plots above, it can be observed the best performing model is model 2 with the highest accuracy level.  Model 2 uses the **Random Forest** algorithm. Although this model may also takes the longest training duration but with computing power nowadays it is possible to further reduce training duration by using higher cores cpu technology and hence achieving an acceptable runtime. For the current weight lifting prediction requirement, model 2 training duration of **`r train_time_elapse[[2]]['elapsed']` sec ( or `r round(train_time_elapse[[2]]['elapsed']/60,2)` mins )** is deemed acceptable.  This best model will be used as the final model to predict the weight lifting actions from the testing dataset.  

----  

###Final Prediction with Chosen Model

```{r final_model}
## Prediction againts test set using best model
final_pred<-predict(model_list[[2]], testing)
final_pred
```

----  

####Appendix : Prediction Result Submission

Creation of the answer files for submission to auto evaluation:

```{r write_result, echo=TRUE}
final_pred_result <- as.character(final_pred)
answers <- final_pred_result

pml_write_files <- function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

if(!dir.exists("./answer")){dir.create("./answer")}
setwd("./answer")
pml_write_files(answers)
```

----    

####Appendix : Analysis Environment

The analysis above was performed with the following system configuration:

```{r sess_info, echo=TRUE}
# Display R session info
sessionInfo()
```

----  



